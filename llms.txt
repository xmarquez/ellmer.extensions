# ellmer.extensions

`ellmer.extensions` extends
[ellmer](https://github.com/tidyverse/ellmer) with:

- [`chat_groq_developer()`](https://xmarquez.github.io/ellmer.extensions/reference/chat_groq_developer.md)
  for Groq structured outputs and Groq batch support.
- [`chat_gemini_extended()`](https://xmarquez.github.io/ellmer.extensions/reference/chat_gemini_extended.md)
  for Gemini chat plus file-based batch support.

The package is fully integrated with ellmer generics like
[`batch_chat()`](https://ellmer.tidyverse.org/reference/batch_chat.html),
[`batch_chat_structured()`](https://ellmer.tidyverse.org/reference/batch_chat.html),
[`parallel_chat()`](https://ellmer.tidyverse.org/reference/parallel_chat.html),
etc. You will need a paid Groq developer account for Groq batch
processing, and a Gemini API key for Gemini batch processing.

## Installation

You can install the development version of `ellmer.extensions` from
GitHub:

``` r
# install.packages("devtools")
devtools::install_github("xmarquez/ellmer.extensions")
```

## Setup

Set one or both provider keys in `.Renviron` (edit with
`usethis::edit_r_environ()`):

``` R
GROQ_API_KEY=your-groq-key
GEMINI_API_KEY=your-gemini-key
```

(`GOOGLE_API_KEY` also works for Gemini.)

## Examples

### Basic Groq chat

``` r
library(ellmer.extensions)

chat_groq <- chat_groq_developer(model = "openai/gpt-oss-20b")
chat_groq$chat("What is the capital of France?")
```

### Basic Gemini chat

``` r
library(ellmer.extensions)

chat_gemini <- chat_gemini_extended(model = "gemini-2.5-flash")
chat_gemini$chat("Reply with exactly one word: hello")
```

### Structured output (Groq)

``` r
type_person <- ellmer::type_object(
  name = ellmer::type_string(),
  age = ellmer::type_integer(),
  city = ellmer::type_string()
)

result <- chat_groq$chat_structured(
  "John is 30 years old and lives in New York City",
  type = type_person
)

str(result)
```

### Batch chat (Groq)

Process multiple requests with cost savings using Groqâ€™s batch API. The
Groq batch API is typically blazingly fast; though completion is not
guaranteed for at least 24 hours, most requests with up to 500 prompts
finish in seconds.

``` r
prompts <- list(
  "What is 2 + 2? Reply with only the number.",
  "What is the capital of New Zealand? Reply with only the city."
)

path <- tempfile(fileext = ".json")

chats <- batch_chat(
  chat_groq,
  prompts = prompts,
  path = path,
  wait = TRUE
)

chats
```

### Batch structured output (Groq)

``` r
type_answer <- ellmer::type_object(
  question = ellmer::type_string(),
  answer = ellmer::type_string()
)

path <- tempfile(fileext = ".json")

data <- batch_chat_structured(
  chat_groq,
  prompts = list("What is 2+2?", "What is the capital of France?"),
  path = path,
  type = type_answer
)

data
```

### Batch chat (Gemini)

Gemini batch processing uses the [file-based Batch
API](https://ai.google.dev/gemini-api/docs/batch-api). Requests are
uploaded as a JSONL file, processed asynchronously at 50% of standard
pricing, with a target 24-hour turnaround (many jobs complete faster).
Maximum input file size is 2 GB.

``` r
prompts <- list(
  "Reply with exactly: ok",
  "Reply with exactly: done"
)

path <- tempfile(fileext = ".json")

# Submit without waiting, then resume later
batch_chat(
  chat_gemini,
  prompts = prompts,
  path = path,
  wait = FALSE
)

batch_chat_completed(chat_gemini, prompts, path)

# Resume and retrieve once completed
chats <- batch_chat(
  chat_gemini,
  prompts = prompts,
  path = path,
  wait = TRUE
)
```

### Parallel chat

Process multiple prompts concurrently (synchronous):

``` r
parallel_chat(
  chat_groq,
  prompts = list(
    "Tell me one fact about Paris.",
    "Tell me one fact about Wellington.",
    "Tell me one fact about Tokyo."
  )
)
```

## Model discovery

- Groq models:
  [`models_groq()`](https://xmarquez.github.io/ellmer.extensions/reference/models_groq.md)
- Gemini models:
  [`ellmer::models_google_gemini()`](https://ellmer.tidyverse.org/reference/chat_google_gemini.html)

## Groq supported models

### Structured outputs

For strict structured outputs, Groq supports [the following
models](https://console.groq.com/docs/structured-outputs#supported-models):

The following models support `strict: true`, which uses constrained
decoding to guarantee schema-compliant output:

| Model ID            | Model                                                                   |
|---------------------|-------------------------------------------------------------------------|
| openai/gpt-oss-20b  | [GPT-OSS 20B](https://console.groq.com/docs/model/openai/gpt-oss-20b)   |
| openai/gpt-oss-120b | [GPT-OSS 120B](https://console.groq.com/docs/model/openai/gpt-oss-120b) |

The following models support Structured Outputs with `strict: false`
(default), which attempts schema compliance but may occasionally error:

| Model ID                                      | Model                                                                                                 |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------|
| openai/gpt-oss-20b                            | [GPT-OSS 20B](https://console.groq.com/docs/model/openai/gpt-oss-20b)                                 |
| openai/gpt-oss-120b                           | [GPT-OSS 120B](https://console.groq.com/docs/model/openai/gpt-oss-120b)                               |
| openai/gpt-oss-safeguard-20b                  | [Safety GPT OSS 20B](https://console.groq.com/docs/model/openai/gpt-oss-safeguard-20b)                |
| moonshotai/kimi-k2-instruct-0905              | [Kimi K2 Instruct](https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct-0905)              |
| meta-llama/llama-4-maverick-17b-128e-instruct | [Llama 4 Maverick](https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct) |
| meta-llama/llama-4-scout-17b-16e-instruct     | [Llama 4 Scout](https://console.groq.com/docs/model/meta-llama/llama-4-scout-17b-16e-instruct)        |

Streaming and tool use are not currently supported with Structured
Outputs on Groq.

### Groq batch processing

For batch processing, [Groq supports the following
models](https://console.groq.com/docs/batch#model-availability-and-pricing):

| Model ID                                      | Model                                                                                                 |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------|
| openai/gpt-oss-20b                            | [GPT-OSS 20B](https://console.groq.com/docs/model/openai/gpt-oss-20b)                                 |
| openai/gpt-oss-120b                           | [GPT-OSS 120B](https://console.groq.com/docs/model/openai/gpt-oss-120b)                               |
| meta-llama/llama-4-maverick-17b-128e-instruct | [Llama 4 Maverick](https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct) |
| meta-llama/llama-4-scout-17b-16e-instruct     | [Llama 4 Scout](https://console.groq.com/docs/model/meta-llama/llama-4-scout-17b-16e-instruct)        |
| llama-3.3-70b-versatile                       | [Llama 3.3 70B](https://console.groq.com/docs/model/llama-3.3-70b-versatile)                          |
| llama-3.1-8b-instant                          | [Llama 3.1 8B Instant](https://console.groq.com/docs/model/llama-3.1-8b-instant)                      |
| meta-llama/llama-guard-4-12b                  | [Llama Guard 4 12B](https://console.groq.com/docs/model/meta-llama/llama-guard-4-12b)                 |

Pricing is at a 50% cost discount compared to [synchronous API
pricing](https://groq.com/pricing). The batch discount does not stack
with [prompt caching](https://console.groq.com/docs/prompt-caching)
discounts; all batch tokens are billed at the 50% batch rate regardless
of cache status.

## Gemini batch processing

Gemini batch processing uses the [file-based Batch
API](https://ai.google.dev/gemini-api/docs/batch-api):

- Requests are uploaded as a JSONL input file, then submitted via
  `batchGenerateContent`.
- Processing is asynchronous at **50% of standard pricing** with a
  target 24-hour turnaround.
- Maximum input file size: **2 GB**.
- Batch jobs expire after **48 hours** if not completed.
- Context caching is supported for batch requests (cache hits are billed
  at standard cache pricing).

Use
[`chat_gemini_extended()`](https://xmarquez.github.io/ellmer.extensions/reference/chat_gemini_extended.md)
to create a Gemini chat object with batch support, then pass it to
[`batch_chat()`](https://ellmer.tidyverse.org/reference/batch_chat.html)
or
[`batch_chat_structured()`](https://ellmer.tidyverse.org/reference/batch_chat.html).
Use
[`batch_chat_completed()`](https://ellmer.tidyverse.org/reference/batch_chat.html)
to check whether a previously submitted batch has finished.

The following [Gemini
models](https://ai.google.dev/gemini-api/docs/models) support the Batch
API:

| Model ID                              |
|---------------------------------------|
| gemini-3-pro-preview                  |
| gemini-3-pro-image-preview            |
| gemini-3-flash-preview                |
| gemini-2.5-flash                      |
| gemini-2.5-flash-preview-09-2025      |
| gemini-2.5-flash-image                |
| gemini-2.5-flash-lite                 |
| gemini-2.5-flash-lite-preview-09-2025 |
| gemini-2.5-pro                        |
| gemini-2.5-pro-preview-tts            |
| gemini-2.0-flash                      |
| gemini-2.0-flash-lite                 |

Use
[`ellmer::models_google_gemini()`](https://ellmer.tidyverse.org/reference/chat_google_gemini.html)
for a full model listing.

## License

MIT

This package was mostly coded by Claude Code, though everything was
reviewed by Xavier Marquez, including all live tests.

# Package index

## All functions

- [`ProviderGroqDeveloper()`](https://xmarquez.github.io/ellmer.extensions/reference/ProviderGroqDeveloper.md)
  : Groq Developer Provider Class
- [`chat_gemini_extended()`](https://xmarquez.github.io/ellmer.extensions/reference/chat_gemini_extended.md)
  : Chat with Gemini Models (Extended Batch Support)
- [`chat_groq_developer()`](https://xmarquez.github.io/ellmer.extensions/reference/chat_groq_developer.md)
  : Chat with Groq AI Models (Developer Version)
- [`models_groq()`](https://xmarquez.github.io/ellmer.extensions/reference/models_groq.md)
  : List Available Groq Models

